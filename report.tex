\documentclass[a4paper,12pt]{article}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\geometry{margin=2cm}
\usepackage{setspace}
\onehalfspacing

\usepackage{hyperref}

\begin{document}

\begin{center}
{\small Министерство науки и высшего образования РФ\\
Федеральное государственное бюджетное образовательное учреждение высшего образования\\
«Московский авиационный институт» (Национальный исследовательский университет)}\\[1cm]

Институт: №8 «Информационные технологии и прикладная математика»\\
Кафедра: 806 «Вычислительная математика и программирование»\\[2cm]

{\Large \textbf{Отчет по лабораторным работам}}\\[0.3cm]
{\large \textbf{по предмету «Информационный поиск»}}\\[3cm]

\begin{flushleft}
Группа: М8О-412Б-22\\
Студент: \textbf{Андриянов Эрик Вячеславович}\\
Оценка: \rule{4cm}{0.4pt}\\
Дата сдачи: \rule{4cm}{0.4pt}\\
\end{flushleft}

\vfill
Москва, 2025 г.
\end{center}

\newpage

\section{Сбор и состав корпуса документов}

\subsection{Постановка задачи}
Первым этапом разработки поисковой системы является формирование корпуса документов. Основными требованиями к корпусу в рамках данной работы были:
\begin{enumerate}
  \item Достаточный объем (не менее 30 000 документов).
  \item Тематическая однородность (область искусственного интеллекта, LLM и AI-агентов).
  \item Наличие минимум трех независимых источников данных.
  \item Поддержка двуязычности (русский и английский языки).
\end{enumerate}

\subsection{Реализация сбора данных}
В качестве источников данных были выбраны:
\begin{itemize}
  \item \textbf{Habr.com} (статьи по тегам AI, нейросети, Python).
  \item \textbf{Arxiv.org} (научные препринты в категории CS.AI).
  \item \textbf{TechCrunch.com} (новости технологий и стартапов в сфере AI).
\end{itemize}

Для сбора данных был разработан многопоточный поисковый робот на языке Python с использованием библиотек \texttt{requests} и \texttt{BeautifulSoup4}. Процесс включал очистку текста от HTML-разметки, навигационных элементов и рекламы.

\subsection{Итоговая статистика}
В результате работы робота был сформирован корпус из 29\,609 документов.
\begin{itemize}
  \item Формат хранения: текстовые файлы \texttt{doc\_XXXXX.txt}.
  \item Общий объем словаря: 736\,796 уникальных словоформ (после токенизации).
\end{itemize}

\section{Архитектура поискового робота}

\subsection{Алгоритм работы}
Разработанный робот функционирует на основе алгоритма обхода в ширину (BFS). Для обеспечения уникальности и предотвращения повторных загрузок реализована система отслеживания состояния:
\begin{itemize}
  \item \texttt{visited\_urls.txt}: файл-реестр всех обработанных ссылок.
  \item Discovery: автоматическое извлечение новых ссылок из текстов статей для углубления поиска.
\end{itemize}

\subsection{Контроль актуальности}
Робот поддерживает механизм возобновления работы: при перезапуске он считывает список уже посещенных URL и продолжает сбор с последнего места, что критично при работе с большими объемами данных.

\section{Лингвистическая обработка}

\subsection{Токенизация}
Реализована на языке C++. Программа разбивает текст на токены, удаляет пунктуацию и спецсимволы, а также корректно обрабатывает многобайтовые последовательности UTF-8 для приведения кириллицы к нижнему регистру.

\subsection{Стемминг}
Для приведения слов к нормальной форме реализован алгоритм Портера на языке C++.
Особенность реализации: алгоритм выполнен без использования стандартной библиотеки шаблонов (STL) для обеспечения максимальной производительности и соблюдения академических требований.
Поддерживаются правила как для русского, так и для английского языков.

\section{Анализ по закону Ципфа}
Для верификации естественности собранного текста был проведен статистический анализ. С помощью скрипта на Python было подсчитано распределение частот слов.
Результатом стал график распределения в логарифмических координатах, подтвердивший линейную зависимость. Наиболее частотные слова — предлоги и технические термины (ии, ai, модел, gpt), что подтверждает репрезентативность корпуса.

\section{Построение инвертированного индекса}
Финальным этапом стало создание структуры данных, позволяющей выполнять мгновенный поиск.
Реализация выполнена на языке C++ без использования STL.
В качестве структуры данных используется собственная реализация хеш-таблицы с методом цепочек для разрешения коллизий.
Формат индекса: бинарный файл \texttt{index.bin}, содержащий словарь терминов и списки документов (postings lists).
Объем словаря в индексе составляет 736\,796 слов.

\section{Результаты тестирования поиска}
Реализована система булева поиска с поддержкой операторов AND и OR.
Система автоматически применяет стемминг к запросу пользователя.

Примеры поисковых запросов:
\begin{itemize}
  \item Запрос: \texttt{AND нейросеть обучение} — найдено 31 документ.
  \item Запрос: \texttt{AND gpt почему} — найдено 625 документов.
  \item Запрос: \texttt{OR интеллект алгоритм} — найдено более 2500 документов.
\end{itemize}

Скорость обработки запроса составляет менее 0{,}01 секунды благодаря использованию бинарного индекса и алгоритма Merge Join для пересечения списков.

\section{Заключение}
В ходе выполнения цикла лабораторных работ была разработана полнофункциональная локальная поисковая система. Пройдены все этапы: от сбора сырых данных из интернета до реализации высокопроизводительного поискового движка на языке C++.
Особое внимание было уделено низкоуровневой реализации алгоритмов (стемминг, хеш-таблицы) без использования сторонних библиотек, что позволило добиться высокой скорости работы и глубокого понимания механизмов индексации данных.

\end{document}

